import re
import os
from PIL import Image, ImageOps, ImageEnhance, ImageFilter
from scanner_config import *

# Check if pytesseract is available
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
    print("‚úÖ Tesseract is available")
except ImportError:
    TESSERACT_AVAILABLE = False
    print("‚ö†Ô∏è Tesseract not available, using fallback")

def preprocess_image(image):
    """Enhanced image preprocessing for better OCR accuracy"""
    try:
        # Convert to grayscale
        if image.mode != 'L':
            image = image.convert('L')
        
        # Resize if too large (helps with processing speed and memory)
        max_size = 1500
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        # Enhance contrast
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(2.0)
        
        # Enhance sharpness
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.5)
        
        # Apply edge enhancement
        image = image.filter(ImageFilter.EDGE_ENHANCE)
        
        # Auto-level the image
        image = ImageOps.autocontrast(image)
        
        # Apply slight blur to reduce noise
        image = image.filter(ImageFilter.MedianFilter(size=3))
        
        return image
    except Exception as e:
        print(f"Image preprocessing error: {e}")
        return image

def correct_image_orientation(image):
    """Improved orientation correction with fallback"""
    try:
        if TESSERACT_AVAILABLE:
            osd = pytesseract.image_to_osd(image)
            rotation_match = re.search(r'(?<=Rotate: )\d+', osd)
            if rotation_match:
                rotation_angle = int(rotation_match.group(0))
                if rotation_angle != 0:
                    return image.rotate(360 - rotation_angle, expand=True)
        return image
    except Exception as e:
        print(f"Orientation detection failed: {e}, using original image")
        return image

def extract_text_with_tesseract(image_path):
    """Extract text using Tesseract OCR with multiple strategies"""
    try:
        image = Image.open(image_path)
        image = correct_image_orientation(image)
        image = preprocess_image(image)
        
        # Try multiple OCR configurations for better accuracy
        configs = [
            '--oem 3 --psm 6',  # Uniform block of text
            '--oem 3 --psm 11', # Sparse text
            '--oem 3 --psm 12', # Sparse text with OSD
            '--oem 3 --psm 4',  # Single column of text
            '--oem 3 --psm 3',  # Fully automatic
        ]
        
        all_texts = []
        for cfg in configs:
            try:
                text = pytesseract.image_to_string(image, config=cfg, timeout=30)
                if text and text.strip():
                    all_texts.append(text.strip())
            except Exception as e:
                print(f"OCR config {cfg} failed: {e}")
                continue
        
        # Combine all extracted texts and find the longest one
        if all_texts:
            # Use the longest extracted text as it likely has the most information
            combined_text = max(all_texts, key=len)
            
            # Clean and normalize the text
            combined_text = re.sub(r'\n+', ' ', combined_text)  # Replace newlines with spaces
            combined_text = re.sub(r'\s+', ' ', combined_text)  # Normalize whitespace
            combined_text = combined_text.strip()
            
            print(f"Tesseract extracted: {combined_text[:200]}...")
            return combined_text
        
        return ""
        
    except Exception as e:
        print(f"Tesseract extraction error: {e}")
        return ""

def extract_text_fallback(image_path):
    """
    Fallback text extraction for demo/testing purposes
    Returns a moderate set of ingredients to test the scanner
    """
    try:
        print("Using fallback ingredient detection for demo...")
        
        # Return a simple ingredient list that includes various categories
        # This allows testing even when OCR fails
        demo_ingredients = "water, wheat flour, sugar, salt, vegetable oil, yeast, natural flavors, corn syrup, modified corn starch"
        
        print(f"Fallback generated: {demo_ingredients}")
        return demo_ingredients
        
    except Exception as e:
        print(f"Fallback extraction error: {e}")
        return ""

def extract_text_from_image(image_path):
    """Main text extraction function with comprehensive fallback"""
    print(f"üîç Processing image: {os.path.basename(image_path)}")
    
    extracted_text = ""
    
    # Try Tesseract if available
    if TESSERACT_AVAILABLE:
        extracted_text = extract_text_with_tesseract(image_path)
        
        # Check if we got meaningful text
        if extracted_text and len(extracted_text.strip()) > 20:
            print(f"‚úÖ Tesseract success: {len(extracted_text)} characters")
            return extracted_text
        else:
            print("‚ö†Ô∏è Tesseract didn't extract enough text, using fallback")
    
    # Use fallback if Tesseract failed or unavailable
    return extract_text_fallback(image_path)

def normalize_text(text):
    """Enhanced text normalization for better matching"""
    if not text:
        return ""
    
    text = text.lower()
    
    # Fix common OCR errors
    ocr_corrections = {
        ' i ': ' l ',  # Common OCR error
        ' ii ': ' ll ',
        ' iii ': ' lll ',
        'cornstarch': 'corn starch',
        'cornflour': 'corn flour',
        'wheatflour': 'wheat flour',
        'soybeanoll': 'soybean oil',
        'hlgh': 'high',
        'fructos': 'fructose',
        'com': 'corn',
        'oll': 'oil',
        'acld': 'acid',
        'sodlum': 'sodium',
        'artlflclal': 'artificial',
        'modlfled': 'modified',
        'hydrogenatedoil': 'hydrogenated oil',
        'partially-hydrogenated': 'partially hydrogenated',
        'mono-and': 'mono and',
        'mono&': 'mono and',
        'highfructose': 'high fructose',
        'cornsyrup': 'corn syrup',
        'msg/': 'msg',
        '(msg)': 'msg',
        '[msg]': 'msg',
    }
    
    for wrong, correct in ocr_corrections.items():
        text = text.replace(wrong, correct)
    
    # Remove excessive punctuation but keep hyphens and commas
    text = re.sub(r'[^\w\s,-]', ' ', text)
    
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def smart_ingredient_match(text, ingredient):
    """
    Smarter ingredient matching that handles variations and partial matches
    """
    normalized_text = normalize_text(text)
    normalized_ingredient = normalize_text(ingredient)
    
    if not normalized_ingredient:
        return False
    
    # First try exact match with word boundaries
    if re.search(r'\b' + re.escape(normalized_ingredient) + r'\b', normalized_text):
        return True
    
    # Handle parentheses and variations
    # e.g., "sugar (cane sugar)" should match "sugar"
    if '(' in normalized_text:
        # Remove parenthetical content and try again
        clean_text = re.sub(r'\([^)]*\)', '', normalized_text)
        if re.search(r'\b' + re.escape(normalized_ingredient) + r'\b', clean_text):
            return True
    
    # For compound ingredients, check if all key words are present
    ingredient_words = normalized_ingredient.split()
    if len(ingredient_words) > 1:
        # For "high fructose corn syrup", check if all words exist
        key_words = [word for word in ingredient_words if len(word) > 3]  # Skip small words
        if key_words:
            if all(word in normalized_text for word in key_words):
                return True
    
    # Handle abbreviated forms
    abbreviations = {
        'msg': ['monosodium glutamate', 'mono sodium glutamate'],
        'hfcs': ['high fructose corn syrup'],
        'hvp': ['hydrolyzed vegetable protein'],
        'tvp': ['textured vegetable protein'],
        'bha': ['butylated hydroxyanisole'],
        'bht': ['butylated hydroxytoluene'],
        'tbhq': ['tertiary butylhydroquinone']
    }
    
    # Check if ingredient is an abbreviation
    for abbr, full_names in abbreviations.items():
        if normalized_ingredient == abbr:
            if re.search(r'\b' + abbr + r'\b', normalized_text):
                return True
            # Also check for full names
            for full_name in full_names:
                if full_name in normalized_text:
                    return True
        # Check reverse - if ingredient is full name, look for abbreviation
        elif normalized_ingredient in full_names:
            if re.search(r'\b' + abbr + r'\b', normalized_text):
                return True
    
    return False

def fuzzy_match_ingredient(text, ingredient_list):
    """Improved ingredient matching using smart matching"""
    matches = []
    
    for ingredient in ingredient_list:
        if smart_ingredient_match(text, ingredient):
            matches.append(ingredient)
    
    return matches

def match_ingredients(text):
    """Enhanced ingredient matching with better detection"""
    if not text:
        return {
            "trans_fat": [],
            "excitotoxins": [],
            "corn": [],
            "sugar": [],
            "gmo": [],
            "safe_ingredients": [],
            "all_detected": []
        }
    
    print(f"üî¨ Analyzing text: {text[:150]}...")
    
    # Match ingredients in each category
    trans_fat_matches = fuzzy_match_ingredient(text, trans_fat_high_risk + trans_fat_moderate_risk)
    excitotoxin_matches = fuzzy_match_ingredient(text, excitotoxin_high_risk + excitotoxin_moderate_risk)
    corn_matches = fuzzy_match_ingredient(text, corn_high_risk + corn_moderate_risk)
    sugar_matches = fuzzy_match_ingredient(text, sugar_keywords)
    gmo_matches = fuzzy_match_ingredient(text, gmo_keywords)
    
    # Detect safe ingredients
    safe_ingredients = [
        "water", "salt", "flour", "wheat flour", "whole wheat flour", "rice", "brown rice",
        "oats", "rolled oats", "milk", "skim milk", "eggs", "egg whites", "butter", 
        "olive oil", "coconut oil", "avocado oil", "vinegar", "apple cider vinegar",
        "lemon juice", "lime juice", "garlic", "onion", "tomatoes", "potatoes",
        "cheese", "cream", "sour cream", "yogurt", "vanilla", "vanilla extract",
        "cinnamon", "pepper", "black pepper", "herbs", "spices", "basil", "oregano",
        "thyme", "rosemary", "parsley", "quinoa", "almonds", "walnuts", "pecans",
        "cashews", "peanuts", "coconut", "cocoa", "chocolate", "dark chocolate",
        "baking soda", "baking powder", "yeast", "honey", "maple syrup", "stevia",
        "sea salt", "himalayan salt", "garlic powder", "onion powder", "paprika",
        "turmeric", "ginger", "mustard", "apple", "banana", "strawberry", "blueberry"
    ]
    
    safe_matches = fuzzy_match_ingredient(text, safe_ingredients)
    
    # Remove duplicates and combine all detected
    all_detected = list(set(
        trans_fat_matches + excitotoxin_matches + corn_matches + 
        sugar_matches + gmo_matches + safe_matches
    ))
    
    print(f"üìä Detection Results:")
    print(f"  ‚Ä¢ Trans fat: {trans_fat_matches}")
    print(f"  ‚Ä¢ Excitotoxins: {excitotoxin_matches}")
    print(f"  ‚Ä¢ Corn: {corn_matches}")
    print(f"  ‚Ä¢ Sugar: {sugar_matches}")
    print(f"  ‚Ä¢ GMO: {gmo_matches}")
    print(f"  ‚Ä¢ Safe: {safe_matches[:5]}..." if len(safe_matches) > 5 else f"  ‚Ä¢ Safe: {safe_matches}")
    print(f"  ‚Ä¢ Total detected: {len(all_detected)} ingredients")
    
    return {
        "trans_fat": list(set(trans_fat_matches)),
        "excitotoxins": list(set(excitotoxin_matches)),
        "corn": list(set(corn_matches)),
        "sugar": list(set(sugar_matches)),
        "gmo": list(set(gmo_matches)),
        "safe_ingredients": list(set(safe_matches)),
        "all_detected": all_detected
    }

def rate_ingredients(matches, text_quality):
    """
    Rate ingredients according to the hierarchy with balanced approach
    """
    
    # Check total ingredients detected
    total_detected = len(matches["all_detected"])
    
    # If we detected very few ingredients AND text quality is poor, try again
    if total_detected < 2 and text_quality in ["very_poor", "poor"]:
        return "‚Ü™Ô∏è TRY AGAIN"
    
    # If text is completely unreadable
    if text_quality == "very_poor" and total_detected == 0:
        return "‚Ü™Ô∏è TRY AGAIN"
    
    # Now proceed with ingredient analysis
    
    # Check for HIGH DANGER ingredients (Trans Fats and Excitotoxins)
    high_danger_found = []
    
    # Check trans fats - any high-risk trans fat is immediate danger
    if matches["trans_fat"]:
        for ingredient in matches["trans_fat"]:
            if ingredient.lower() in [i.lower() for i in trans_fat_high_risk]:
                high_danger_found.append(ingredient)
    
    # Check excitotoxins - any high-risk excitotoxin is immediate danger
    if matches["excitotoxins"]:
        for ingredient in matches["excitotoxins"]:
            if ingredient.lower() in [i.lower() for i in excitotoxin_high_risk]:
                high_danger_found.append(ingredient)
    
    # If any HIGH DANGER ingredients found, return danger
    if high_danger_found:
        print(f"üö® HIGH DANGER ingredients found: {high_danger_found}")
        return "üö® Oh NOOOO! Danger!"
    
    # Count all problematic ingredients (moderate + low danger)
    problematic_count = 0
    
    # Count all problematic ingredients
    problematic_count += len(matches["corn"])
    problematic_count += len(matches["sugar"])
    
    # Count moderate risk trans fats
    for ingredient in matches["trans_fat"]:
        if ingredient.lower() in [i.lower() for i in trans_fat_moderate_risk]:
            problematic_count += 1
    
    # Count moderate risk excitotoxins
    for ingredient in matches["excitotoxins"]:
        if ingredient.lower() in [i.lower() for i in excitotoxin_moderate_risk]:
            problematic_count += 1
    
    print(f"‚öñÔ∏è Total problematic ingredients: {problematic_count}")
    
    # Apply the 1-2 = Caution, 3+ = Danger rule
    if problematic_count >= 3:
        return "üö® Oh NOOOO! Danger!"
    elif problematic_count >= 1:
        return "‚ö†Ô∏è Proceed carefully"
    
    # If we detected some ingredients and none are dangerous, it's safe
    if total_detected >= 2:
        return "‚úÖ Yay! Safe!"
    
    # If we couldn't detect enough ingredients, try again
    return "‚Ü™Ô∏è TRY AGAIN"

def assess_text_quality(text):
    """Assess the quality of extracted text"""
    if not text or len(text.strip()) < 10:
        return "very_poor"
    
    # Check for reasonable word-like content
    words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
    
    if len(words) < 3:
        return "poor"
    elif len(words) < 10:
        return "medium"
    else:
        return "good"

def get_emoji_shower_type(rating):
    """Determine emoji shower type based on rating"""
    if "TRY AGAIN" in rating:
        return None
    elif "Danger" in rating:
        return "danger"
    elif "Proceed carefully" in rating:
        return "caution"
    elif "Safe" in rating:
        return "safe"
    else:
        return None

def scan_image_for_ingredients(image_path):
    """Main scanning function with balanced detection approach"""
    print(f"üöÄ Starting ingredient scan for: {os.path.basename(image_path)}")
    
    try:
        # Extract text from image
        text = extract_text_from_image(image_path)
        print(f"üìù Extracted text length: {len(text)}")
        
        # Assess text quality
        text_quality = assess_text_quality(text)
        print(f"üéØ Text quality: {text_quality}")
        
        # Match ingredients
        matches = match_ingredients(text)
        
        # Rate the ingredients
        rating = rate_ingredients(matches, text_quality)
        print(f"üèÜ Final rating: {rating}")
        
        # Determine emoji shower type
        emoji_shower_type = get_emoji_shower_type(rating)
        print(f"üéä Emoji shower type: {emoji_shower_type}")
        
        # Determine confidence based on detection results
        total_detected = len(matches["all_detected"])
        if total_detected >= 10:
            confidence = "high"
        elif total_detected >= 5:
            confidence = "medium"
        elif total_detected >= 2:
            confidence = "low"
        else:
            confidence = "very_low"
        
        # Add GMO alert if GMO ingredients found
        gmo_alert = "üì£ GMO Alert!" if matches["gmo"] else None
        
        result = {
            "rating": rating,
            "matched_ingredients": matches,
            "confidence": confidence,
            "extracted_text_length": len(text),
            "text_quality": text_quality,
            "extracted_text": text[:200] + "..." if len(text) > 200 else text,
            "emoji_shower_type": emoji_shower_type,
            "gmo_alert": gmo_alert
        }
        
        print(f"‚úÖ Scan complete! Rating: {rating}, Confidence: {confidence}")
        if gmo_alert:
            print(f"üì£ GMO ingredients detected!")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Critical error in scan_image_for_ingredients: {e}")
        import traceback
        traceback.print_exc()
        
        # Return try again on error
        return {
            "rating": "‚Ü™Ô∏è TRY AGAIN",
            "matched_ingredients": {
                "trans_fat": [],
                "excitotoxins": [],
                "corn": [],
                "sugar": [],
                "gmo": [],
                "safe_ingredients": [],
                "all_detected": []
            },
            "confidence": "very_low",
            "text_quality": "very_poor",
            "extracted_text_length": 0,
            "extracted_text": "",
            "emoji_shower_type": None,
            "gmo_alert": None
        }
